
aws configure (for re-invoke information caching)
aws efs create-file-system --creation-token $(uuid)    
-----
{
    "SizeInBytes": {
        "Value": 0
    },
    "ThroughputMode": "bursting",
    "CreationToken": "d68b03fe-ca41-11e8-80a5-02e4173a6bfe",
    "Encrypted": false,
    "CreationTime": 1538924476.0,
    "PerformanceMode": "generalPurpose",
    "FileSystemId": "fs-fc0e1b55",
    "NumberOfMountTargets": 0,
    "LifeCycleState": "creating",
    "OwnerId": "495627566033"
}

+ aws efs create-mount-target \
        --file-system-id fs-fc0e1b55 \
        --subnet-id subnet-019427bc1d90f1091 \
        --security-groups sg-04b62fc4af711d0e3
-----
{
    "MountTargetId": "fsmt-5ab5e4f3",
    "NetworkInterfaceId": "eni-04414b3844725e0ca",
    "FileSystemId": "fs-fc0e1b55",
    "LifeCycleState": "creating",
    "SubnetId": "subnet-019427bc1d90f1091",
    "OwnerId": "495627566033",
    "IpAddress": "172.20.58.110"
}


check until LifeCycleState=available
+ aws efs describe-mount-targets --file-system-id fs-fc0e1b55 



aws ec2 authorize-security-group-ingress --group-id sg-04b62fc4af711d0e3 --protocol tcp --port 2049 --source-group sg-04b62fc4af711d0e3 --group-owner 495627566033


+ vi efs-provisioner.yaml --> 
    roleRef:
     kind: ClusterRole
     #name: efs-provisioner-runner <-- this one does not have permission 
     #                               to get endpoint in name space kube-system
     name: cluster-admin   

+ kubectl apply -f efs-provisioner.yaml
+ kubectl get pvc   
  --> efs STATUS is Bound -> ok


#-----------------------------------------------#
#                SECOND EFS                     #
#-----------------------------------------------#
aws efs create-file-system --creation-token $(uuid)
{
    "SizeInBytes": {
        "Value": 0
    },
    "CreationToken": "acb15680-a95a-11e8-abbc-024ae44c014a",
    "Encrypted": false,
    "CreationTime": 1535306754.0,
    "PerformanceMode": "generalPurpose",
    "FileSystemId": "fs-94eab33d",
    "NumberOfMountTargets": 0,
    "LifeCycleState": "creating",
    "OwnerId": "49"
}


+ aws efs create-mount-target \
        --file-system-id fs-94eab33d \
        --subnet-id subnet-019427bc1d90f1091 \
        --security-groups sg-04b62fc4af711d0e3
---
{
    "MountTargetId": "fsmt-d0bafb79",
    "NetworkInterfaceId": "eni-0d356cae5104b5b0c",
    "FileSystemId": "fs-94eab33d",
    "LifeCycleState": "creating",
    "SubnetId": "subnet-019427bc1d90f1091",
    "OwnerId": "495627566033",
    "IpAddress": "172.20.59.233"
}


+ aws ec2 authorize-security-group-ingress --group-id sg-04b62fc4af711d0e3 --protocol tcp --port 2049 --source-group sg-04b62fc4af711d0e3 --group-owner 495627566033

#---------------------------------------#
#               INGRESS                 #
#---------------------------------------#
kb create -f install/common/ns-and-sa.yaml
kb create -f install/common/default-server-secret.yaml
kb create -f install/common/nginx-config.yaml
kb create -f install/rbac/rbac.yaml
kb create -f install/deployment/nginx-ingress.yaml
kb create -f install/service/loadbalancer-aws-elb.yaml

kb describe pods nginx-ingress-89f96847c-vpjvf --namespace=nginx-ingress
kb get ingress,nodes,pods,services,deployments --all-namespaces
kb get all --all-namespaces


"Attach to ingress-control to verify rules"
kubectl exec -it <pod_name> bash --namespace=<name_space>
kubectl exec -it  nginx-ingress-89f96847c-vpjvf bash --namespace=nginx-ingress


---
kubectl create -f namespace.yaml
kubectl create -f rbac.yaml
kubectl create -f default-backend.yaml           
kubectl create -f nginx-ingress-controller.yaml  
                 This will create a deployment whose pods will have the ports 80 and 443 open for http and https respectively. 
kubectl create -f nginx-controller-service.yaml
#This expose this deployment so that it will have External IP through which users will connect to our app
kubectl apply -f nginx-contoller-patch.yaml
kubectl get pods --namespace=ingress-nginx-config
kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
kubectl get nodes -o yaml | grep ExternalIP -C 1
kubectl get pods -o yaml | grep podIP



#---------------------------------------#
#         COMMON DEBUGGING CLI          #
#---------------------------------------#

# Check the status of control plan (master node)
kubectl get componentstatuses

# Get ALL pods/deployement/services/nodes
kb get ingress,nodes,pods,services,deployments --all-namespaces
kb get all --all-namespaces

# Attach to container for system hands-on
kubectl exec -it <pod_name> bash --namespace=<name_space>
Ex: kubectl exec -it  nginx-ingress-89f96847c-vpjvf bash --namespace=nginx-ingress
    kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE

# Copy file inside container
kubectl cp <pod-name>:/path_to_remote_file /path_to_local_file

# Export information about pods/deployment/services/nodes into YAML,JSON,...
kubectl get nodes -o yaml | grep ExternalIP -C 1
kubectl get pods -o yaml | grep podIP

# Export information with COLUMN Customization
kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system

# View resource usage on each pod/node
kb top pods/nodes
